{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch-BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#[BERT](https://arxiv.org/abs/1810.04805)\n",
        "\n",
        "BERT(Bidirectional Encoder Representations from Transformers) has the following components:\n",
        "\n",
        "1. Embedding layers\n",
        "2. Attention Mask\n",
        "3. Encoder layer\n",
        "   1. Multi-head attention\n",
        "      1. Scaled dot product attention\n",
        "      2. Position-wise feed-forward network\n",
        "4. BERT (assembling all the components"
      ],
      "metadata": {
        "id": "yWi871cBbLPH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S_GVrv_abzx",
        "outputId": "b4c1c21c-b40d-4582-f887-1a2b74753b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODULES LOADED\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import re\n",
        "from random import *\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "print(\"MODULES LOADED\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding Layer\n",
        "\n",
        "The embedding is the first layer in BERT that takes the input and creates a lookup table. The parameters of the embedding layers are learnable, which means when the learning process is over the embeddings will cluster similar words together. \n",
        "\n",
        "The embedding layer also preserves different relationships between words such as: semantic, syntactic, linear, and since BERT is bidirectional it will also preserve contextual relationships as well. \n",
        "\n",
        "In the case of BERT, it creates three embeddings for \n",
        "\n",
        "Token,Position and Segments.\n",
        "\n",
        "If you recall we havenâ€™t created a function that takes the input and formats it for position embedding but the formatting for token and segments are completed. So we will take the input and create a position for each word in the sequence"
      ],
      "metadata": {
        "id": "bkEb8NMjbIye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingLayer(nn.Module):\n",
        "   def __init__(self):\n",
        "      super(EmbeddingLayer,self).__init__()\n",
        "      self.token_embedding = nn.Embedding(vocab_size,d_model)\n",
        "      self.position_embedding = nn.Embedding(maxlen,d_model)\n",
        "      self.segment_embedding = nn.Embedding(n_segments,d_model)\n",
        "      self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "   def forward(self,x,seg):\n",
        "      seq_len = x.size(1)\n",
        "      pos = torch.arange(seq_len,dtype=torch.long)  \n",
        "      #(seq_len,) --> (batch_size,seq_len)\n",
        "      pos = pos.unsqueeze(0).expand_as(x)\n",
        "      embedding = self.token_embedding(x) + self.position_embedding(pos) + self.segment_embedding(seg)\n",
        "      return self.norm(embedding)"
      ],
      "metadata": {
        "id": "F84pzb0qazDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention Mask\n",
        "Attention masks allow us to send a batch into the transformer even when the examples in the batch have varying lengths.\n",
        "\n",
        "eg:\n",
        "[CLS] The cat is walking [PAD] [PAD] [PAD]. [CLS] The dog is barking at the tree.\n",
        "\n",
        "The length of the first sentence is equal to the length of the second sentence."
      ],
      "metadata": {
        "id": "2gzn8TcmjuOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_pad_mask(seq_q,seq_k):\n",
        "    batch_size,len_q = seq_q.size()\n",
        "    batch_size,len_k = seq_k.size()\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
      ],
      "metadata": {
        "id": "22GY66AOf5zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder Layer\n",
        "The encoder has two main components: \n",
        "\n",
        "1. Multi-head Attention\n",
        "2. Position-wise feed-forward network.\n",
        " \n",
        "The work of the encoder is to find representations and patterns from the input and attention mask."
      ],
      "metadata": {
        "id": "sKiNqM_Oky4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "   def __init__(self):\n",
        "      super(EncoderLayer,self).__init__()\n",
        "      self.enc_self_attn = MultiHeadAttention()\n",
        "      self.pos_ffn = PositionWiseFeedForward()\n",
        "\n",
        "   def forward(enc_inputs,enc_self_attn_mask):\n",
        "      enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "      enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "      return enc_outputs, attn"
      ],
      "metadata": {
        "id": "JRjptp6KkJlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi Head Attention\n",
        "\n",
        "This is the first of the main components of the encoder. \n",
        "\n",
        "The attention model takes three inputs: Query, Key, and Value.  \n",
        "\n",
        "Multihead attention takes four inputs: Query, Key, Value, and Attention mask. The embeddings are fed as input to the Query, Key, and Value argument, and the attention mask is fed as input to the attention mask argument. \n",
        "These three inputs and the attention mask are operated with a dot product operation that yields two outputs: context vectors and attention. The context vector is then passed through a linear layer and finally that yields the output."
      ],
      "metadata": {
        "id": "PCvQl_o2nsjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "   def __init__(self):\n",
        "      super(MultiHeadAttention,self).__init__()\n",
        "      self.WQ = nn.Linear(d_model,d_k * n_heads)\n",
        "      self.WK = nn.Linear(d_model,d_k * n_heads)\n",
        "      self.WV = nn.Linear(d_model,d_v * n_heads)\n",
        "\n",
        "   def forward(self,Q,K,V,attn_mask):\n",
        "      # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "      residual,batch_size = Q,Q.size(0)\n",
        "      # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "      q_s = self.WQ(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "      k_s = self.WK(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "      v_s = self.WV(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "      \n",
        "      # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "      attn_mask = attn_mask.unsqueeze(1).repeat(1,n_heads,1,1)\n",
        "\n",
        "      # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "      context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "      context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "      output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "\n",
        "      return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]"
      ],
      "metadata": {
        "id": "ffEqIuEnlCy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scaled Dot Product Attention\n",
        "The scaled dot product attention class takes four arguments: Query, Key, Value, and Attention mask. Essentially, the first three arguments are fed with the word embeddings and the attention mask argument is fed with attention mask embeddings.\n",
        "Then it does a matrix multiplication between query and key to get scores. \n",
        "Following that we use scores.masked_fill_(attn_mask, -1e9) . This attribute fills the element of scores with -1e9 where the attention masks are True while the rest of the elements get an attention score which is then passed through a softmax function that gives a score between 0 and 1. Finally, we perform a matrix multiplication between attention and values which gives us the context vectors."
      ],
      "metadata": {
        "id": "eogf0Mhd-_uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "   def __init__(self):\n",
        "       super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "   def forward(self, Q, K, V, attn_mask):\n",
        "       scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "       scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "       attn = nn.Softmax(dim=-1)(scores)\n",
        "       context = torch.matmul(attn, V)\n",
        "       return score, context, attn"
      ],
      "metadata": {
        "id": "JnE4xzhq7iO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Position Wise Feed-Forward Layer"
      ],
      "metadata": {
        "id": "KkHhtA6f_sqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
        "        return self.fc2(gelu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "DPFVVd1L_OYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GeLU Activation"
      ],
      "metadata": {
        "id": "Pa4pwIfeAKsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "    \"Implementation of the GeLU activation function\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "metadata": {
        "id": "3GtVVmes_50U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Batches"
      ],
      "metadata": {
        "id": "Yeh2VactAdjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch():\n",
        "    batch = []\n",
        "    positive = negative = 0\n",
        "    while positive != batch_size/2 or negative != batch_size/2:\n",
        "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n",
        "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        # MASK LM\n",
        "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
        "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
        "        shuffle(cand_maked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        for pos in cand_maked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            if random() < 0.8:  # 80%\n",
        "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
        "            elif random() < 0.5:  # 10%\n",
        "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
        "                input_ids[pos] = word_dict[number_dict[index]] # replace\n",
        "\n",
        "        # Zero Paddings\n",
        "        n_pad = maxlen - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "        # Zero Padding (100% - 15%) tokens\n",
        "        if max_pred > n_pred:\n",
        "            n_pad = max_pred - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
        "            negative += 1\n",
        "    return batch"
      ],
      "metadata": {
        "id": "_Q9R7QmcAc2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Class"
      ],
      "metadata": {
        "id": "JXJGDMVUDY66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(nn.Module):\n",
        "   def __init__(self):\n",
        "      super(BERT,self).__init__()\n",
        "      self.embedding = EmbeddingLayer()\n",
        "      self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "      self.fc = nn.Linear(d_model, d_model)\n",
        "      self.activ1 = nn.Tanh()\n",
        "      self.linear = nn.Linear(d_model, d_model)\n",
        "      self.activ2 = gelu\n",
        "      self.norm = nn.LayerNorm(d_model)\n",
        "      self.classifier = nn.Linear(d_model, 2)\n",
        "      # decoder is shared with embedding layer\n",
        "      embed_weight = self.embedding.token_embedding.weight\n",
        "      n_vocab, n_dim = embed_weight.size()\n",
        "      self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "      self.decoder.weight = embed_weight\n",
        "      self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "\n",
        "   def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
        "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
        "\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
        "        # get masked position from final output of transformer.\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_clsf"
      ],
      "metadata": {
        "id": "tow41RoBA3a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # BERT Parameters\n",
        "    maxlen = 30 # maximum length\n",
        "    batch_size = 6\n",
        "    max_pred = 5  # max tokens of prediction\n",
        "    n_layers = 12 # number of Encoder Layer\n",
        "    n_heads = 12 # number of heads in Multi-Head Attention\n",
        "    d_model = 768 # Embedding Size\n",
        "    d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
        "    d_k = d_v = 64  # dimension of K(=Q), V\n",
        "    n_segments = 2\n",
        "\n",
        "    text = (\n",
        "        'Hello, how are you? I am Romeo.\\n'\n",
        "        'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
        "        'Nice meet you too. How are you today?\\n'\n",
        "        'Great. My baseball team won the competition.\\n'\n",
        "        'Oh Congratulations, Juliet\\n'\n",
        "        'Thanks you Romeo'\n",
        "    )\n",
        "    sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
        "    word_list = list(set(\" \".join(sentences).split()))\n",
        "    word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "    for i, w in enumerate(word_list):\n",
        "        word_dict[w] = i + 4\n",
        "    number_dict = {i: w for i, w in enumerate(word_dict)}\n",
        "    vocab_size = len(word_dict)\n",
        "\n",
        "    token_list = list()\n",
        "    for sentence in sentences:\n",
        "        arr = [word_dict[s] for s in sentence.split()]\n",
        "        token_list.append(arr)\n",
        "\n",
        "    model = BERT()"
      ],
      "metadata": {
        "id": "54azntxCDcTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggYvzGgGGsxl",
        "outputId": "8bd233d8-4a16-4748-c85f-034214da963e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT(\n",
              "  (embedding): EmbeddingLayer(\n",
              "    (token_embedding): Embedding(29, 768)\n",
              "    (position_embedding): Embedding(30, 768)\n",
              "    (segment_embedding): Embedding(2, 768)\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (layers): ModuleList(\n",
              "    (0): EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (WQ): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WK): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WV): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (1): EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (WQ): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WK): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WV): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (2): EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (WQ): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WK): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WV): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (3): EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (WQ): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WK): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WV): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (4): EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (WQ): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WK): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WV): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (5): EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (WQ): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WK): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WV): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (6): EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (WQ): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WK): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WV): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (7): EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (WQ): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WK): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WV): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (8): EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (WQ): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WK): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WV): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (9): EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (WQ): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WK): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WV): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (10): EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (WQ): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WK): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WV): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (11): EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (WQ): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WK): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (WV): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (activ1): Tanh()\n",
              "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (decoder): Linear(in_features=768, out_features=29, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "usPXCyTjIqxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "batch = make_batch()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "    loss_lm = (loss_lm.float()).mean()\n",
        "    loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
        "    loss = loss_lm + loss_clsf\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "         print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
        "print(text)\n",
        "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
        "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
        "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
        "\n",
        "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
        "print('isNext : ', True if isNext else False)\n",
        "\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_clsf else False)"
      ],
      "metadata": {
        "id": "qt7N3rNiFlUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wGg-EI8FKFmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bert Base Uncased"
      ],
      "metadata": {
        "id": "BwKwvh9gKPzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "bI8PrliPGG-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "blscNS4iKHcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zwvYR9qKMfY",
        "outputId": "a86e5b61-5da7-4f98-e510-cee5cda3554a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iqJ6DOGuKOUW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}